{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic: Machine Learning from Disaster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Una solución usando árboles aleatorios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta libreta se explicará por partes una solución para el problema del Titanic usando árboles de decisión.\n",
    "El código original puede ser consultado en la siguiente página:\n",
    "\n",
    "https://www.kaggle.com/kushal1412/titanic/decision-tree-survivors/code\n",
    "\n",
    "El autor del código no lo explica a detalle pero es fácil de comprender con la información que se tiene del problema y usando como referencia la documentación siguiente de sklearn:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "\n",
    "\n",
    "\n",
    "Comenzamos importando las librerias necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import cross_validation as cv\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier                         # KNN\n",
    "from sklearn.svm import SVC                                                # SVM\n",
    "from sklearn.ensemble import RandomForestClassifier                        # Bósque aleatorios\n",
    "from sklearn.ensemble import AdaBoostClassifier                            # ADA Boost\n",
    "from sklearn.naive_bayes import GaussianNB                                 # Naive bayes\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis       # Logística sin regularización\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis    # Logística con polinomio de orden 2\n",
    "from sklearn.linear_model import  LogisticRegression                       # Logística con regularización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego sigue importar los datos de entrenamiento y los de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consiguiendo los datos de entrenamiento y el conjunto de prueba...\n",
      "Datos de entrenamiento: 891\n",
      "Datos de prueba: 418\n"
     ]
    }
   ],
   "source": [
    "print('Consiguiendo los datos de entrenamiento y el conjunto de prueba...')\n",
    "train = pd.read_csv(\"train.csv\", dtype={\"Age\": np.float64})\n",
    "test  = pd.read_csv(\"test.csv\", dtype={\"Age\": np.float64})\n",
    "\n",
    "print 'Datos de entrenamiento:', len(train)\n",
    "\n",
    "print 'Datos de prueba:', len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tendremos una función que \"armoniza\" los datos. Su trabajo es llenar campos vacíos y asignar un valor numérico a los datos que no lo sean (sexo y lugar de embarcación)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def harmonize_data(titanic):\n",
    "    # Llena campos vacíos\n",
    "    titanic[\"Age\"] = titanic[\"Age\"].fillna(titanic[\"Age\"].mean())\n",
    "    titanic[\"Fare\"] = titanic[\"Fare\"].fillna(titanic[\"Fare\"].mean())\n",
    "    titanic[\"Embarked\"] = titanic[\"Embarked\"].fillna(\"S\")\n",
    "    # Asigna valores numéricos a los datos para facilitar cálculos\n",
    "    titanic.loc[titanic[\"Sex\"] == \"male\", \"Sex\"] = 1\n",
    "    titanic.loc[titanic[\"Sex\"] == \"female\", \"Sex\"] = 0\n",
    "    titanic.loc[titanic[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n",
    "    titanic.loc[titanic[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n",
    "    titanic.loc[titanic[\"Embarked\"] == \"Q\", \"Embarked\"] = 2\n",
    "    return titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto evitaremos errores que pudiera causar un campo vacio.\n",
    "\n",
    "Usamos la función en todos los datos que usaremos para el entrenamiento y el proceso de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = harmonize_data(train)\n",
    "test_data  = harmonize_data(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De una vez declaramos la función que usaremos más adelante para guardar nuestrar predicciones en un archivo de Excel. En la primera columna se muestra el ID de cada pasajero y a su derecha la predicción de si sobrevivirán o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_submission(dtc, train, test, predictors, filename):\n",
    "    dtc.fit(train[predictors], train[\"Survived\"])\n",
    "    predictions = dtc.predict(test[predictors])\n",
    "    submission = pd.DataFrame({\n",
    "        \"PassengerId\": test[\"PassengerId\"],\n",
    "        \"Survived\": predictions\n",
    "    })\n",
    "    submission.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con nuestros datos ya procesados, agregaremos un par de campos.\n",
    "* El campo PSA es el producto del valor de clase, sexo y edad del pasajero. Recordemos que a los campos de clase y sexo se les asignó un valor numérico.\n",
    "* El campo SP contiene el número de familiares del pasajero a bordo, dentro de los cuales se toman en cuenta hijos, esposos, hermanos y padres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data[\"PSA\"] = train_data[\"Pclass\"]*train_data[\"Sex\"]*train_data[\"Age\"]\n",
    "train_data[\"SP\"] = train_data[\"SibSp\"]+train_data[\"Parch\"]\n",
    "test_data[\"PSA\"] = test_data[\"Pclass\"]*test_data[\"Sex\"]*test_data[\"Age\"]\n",
    "test_data[\"SP\"] = test_data[\"SibSp\"]+test_data[\"Parch\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora fijaremos nuestros \"predictores\", que serán los atributos que tomaremos en cuenta de cada pasajero para calcular predicciones. Estos campos son:\n",
    "* Clase\n",
    "* Sexo\n",
    "* Edad\n",
    "* PSA\n",
    "* Cantidad que pagó por el boleto\n",
    "* Lugar de embarcación\n",
    "* SP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"PSA\", \"Fare\", \"Embarked\", \"SP\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En *sklearn* los árboles incluyen la función *score*, la cual regresa la precisión de los datos de prueba, dando como parámetros estos datos y las clases a las que pertenece cada uno. Usaremos esto para calcular la profundidad (llamada *max_depth* por el algoritmo) que nos dé los resultados más acertados, probando cada caso del 1 al 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 0.8205118601747815)\n"
     ]
    }
   ],
   "source": [
    "max_score = 0\n",
    "best_n = 0\n",
    "for n in range(1,100):\n",
    "    dtc_scr = 0.\n",
    "    dtc = DecisionTreeClassifier(max_depth=n)\n",
    "    for train, test in KFold(len(train_data), n_folds=10, shuffle=True):\n",
    "        dtc.fit(train_data[predictors].T[train].T, train_data[\"Survived\"].T[train].T)\n",
    "        dtc_scr += dtc.score(train_data[predictors].T[test].T, train_data[\"Survived\"].T[test].T)/10\n",
    "    if dtc_scr > max_score:\n",
    "        max_score = dtc_scr\n",
    "        best_n = n\n",
    "print(best_n, max_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro parámetro del arbol de decisión es *min_samples_split*, el cual es el mínimo número de muestras necesarias para partir un nodo. Al igual que con la profundidad, buscaremos el mejor valor para nuestro algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23, 0.83389513108614222)\n"
     ]
    }
   ],
   "source": [
    "max_score = 0\n",
    "best_s = 0\n",
    "for s in range(1,100):\n",
    "    dtc_scr = 0.\n",
    "    dtc = DecisionTreeClassifier(min_samples_split=s)\n",
    "    for train, test in KFold(len(train_data), n_folds=10, shuffle=True):\n",
    "        dtc.fit(train_data[predictors].T[train].T, train_data[\"Survived\"].T[train].T)\n",
    "        dtc_scr += dtc.score(train_data[predictors].T[test].T, train_data[\"Survived\"].T[test].T)/10\n",
    "    if dtc_scr > max_score:\n",
    "        max_score = dtc_scr\n",
    "        best_s = s\n",
    "print(best_s, max_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de esto ya podemos crear nuestro arbol de decisión. El código original usaba como criterio *entropy* y un separador de nodos *random*. El criterio puede cambiarse a *gini* para usar la impureza de Gini en vez de ganancia de información, y como estrategia para el separador se puede cambiar a *best* para elegir la mejor separación (*random* usa la mejor separación aleatoria)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Haciendo predicciones...\n",
      "Creando archivo Excel...\n",
      "Listo.\n"
     ]
    }
   ],
   "source": [
    "print('Haciendo predicciones...')\n",
    "dtc = DecisionTreeClassifier(max_depth=best_n, min_samples_split=best_s, criterion='entropy', splitter='random')\n",
    "print('Creando archivo Excel...')\n",
    "create_submission(dtc, train_data, test_data, predictors, \"dtcsurvivors.csv\")\n",
    "print('Listo.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a hacer unas modificaciones al algoritmo para ver cómo cambian los resultados. Cambiaremos el criterio al *default* y el separador a *best*; o lo que es lo mismo: no asignarles valor y usarán estos por default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Haciendo predicciones...\n",
      "Creando archivo Excel...\n",
      "Listo.\n"
     ]
    }
   ],
   "source": [
    "print('Haciendo predicciones...')\n",
    "dtc = DecisionTreeClassifier(max_depth=best_n, min_samples_split=best_s)\n",
    "print('Creando archivo Excel...')\n",
    "create_submission(dtc, train_data, test_data, predictors, \"dtcsurvivors2.csv\")\n",
    "print('Listo.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
